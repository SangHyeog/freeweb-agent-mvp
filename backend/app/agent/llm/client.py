# Provide Ïä§ÏúÑÏπò Í∞ÄÎä•Ìïú ÏµúÏÜå ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Optional, Tuple

from app.agent.llm.prompts import SYSTEM_PROMPT_DIFF_ONLY
from app.agent.llm.diff_guard import normalize_diff, validate_unified_diff
from app.agent.llm.errors import LLMError


@dataclass
class LLMConfig:
    provider: str
    model: str
    temperature: float = 0.0
    max_tokens: int = 1200


def load_llm_config() -> LLMConfig:
    provider = os.getenv("LLM_PROVIDER", "openai").lower()
    temperature = float(os.getenv("LLM_TEMPERATURE", "0"))
    max_tokens = int(os.getenv("LLM_MAX_TOKENS", "1200"))

    if provider == "anthropic":
        model = os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonet-20241022")
    elif provider == "openai":
        model = os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
    elif provider == "gemini":
        model = os.getenv("GEMINI_MODEL", "models/gemini-2.5-pro")
    else:
        raise LLMError(f"Unknown LLM_PROVIDER={provider}")

    return LLMConfig(
        provider=provider,
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
    )


def generate_fix_diff(*, error_log: str, files: list[dict]) -> Tuple[Optional[str], bool]:
    """
    Returns:
      diff: unified diff string or None
      estimated: True if fallback / heuristic was used
    """
    cfg = load_llm_config()

    try:
        if cfg.provider == "anthropic":
            diff =  _anthropic_generate_diff(
                cfg,
                error_log=error_log,
                files=files,
            )
            return diff, False

        if cfg.provider == "openai":
            diff = _openai_generate_diff(
                cfg,
                error_log=error_log,
                files=files,
            )
            return diff, False

        if cfg.provider == "gemini":
            diff = _gemini_generate_diff(
                cfg,
                error_log=error_log,
                files=files,
            )
            return diff, False
            
        raise LLMError(f"Unsupported LLM_PROVIDER={cfg.provider}")

    except Exception as e:
        # üëá Ïó¨Í∏∞ÏÑú LLM Ïã§Ìå®Î•º Ìù°Ïàò
        llm_error_msg = str(e)

    # ---- FALLBACK ZONE (Day23 ÌïµÏã¨) ----
    # Í∑úÏπô Í∏∞Î∞ò / stub / offline ÎåÄÏùë
    for f in files:
        """
        print("==== FALLBACK CHECK ====")
        print("error_log repr:", repr(error_log))
        print("file path:", f["path"])
        print("file content repr:", repr(f["content"]))
        print("cond1 ReferenceError:", "ReferenceError" in error_log)
        print("cond2 test1 is not defined:", "test1 is not defined" in error_log)
        print("cond3 console.log(test1:", "console.log(test1" in f["content"])
        print("========================")
        """
        if (
            "ReferenceError" in error_log
            and "test1 is not defined" in error_log
            and "console.log(test1" in f["content"]["content"]
        ):
            diff = (
                f"--- a/{f['path']}\n"
                f"+++ b/{f['path']}\n"
                f"@@ -2,1 +2,1 @@\n"
                f"-console.log(test1);\n"
                f"+console.log(\"test1\");\n"
            )
            return diff, True
        
    # preview Îã®Í≥ÑÏóêÏÑúÎäî Îπà diffÎùºÎèÑ Î∞òÌôò
    return None, True   # True = estimated


def _build_files_block(files: list[dict]) -> str:
    blocks = []
    for f in files:
        blocks.append(
            f"""File: {f['path']}
----------------
{f['content']}
----------------
"""
        )
    return "\n\n".join(blocks)


def _openai_generate_diff(cfg: LLMConfig, *, error_log: str, files: list[dict]) -> str:
    # OpenAI Python SDK v1 Ïä§ÌÉÄÏùº Í∞ÄÏ†ï
    from openai import OpenAI

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise LLMError("OPENAI_API_KEY is misssing")
    
    client = OpenAI(api_key=api_key)

    files_block = _build_files_block(files)

    user_prompt = f"""\
Error log:
{error_log}

Related files:
{files_block}

Generate a unified diff that fixes the error.
Output ONLY unified diff.
"""
    resp = client.chat.completions.create(
        model=cfg.model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT_DIFF_ONLY},
            {"role": "user", "content": user_prompt},
        ],
        temperature=cfg.temperature,
        max_tokens=cfg.max_tokens,
    )

    text = resp.choices[0].message.content or ""
    text = normalize_diff(text)
    validate_unified_diff(text)
    return text


def _anthropic_generate_diff(cfg: LLMConfig, *, error_log: str, files: list[dict]) -> str:
    # Anthropic Python SDK
    # Í≥µÏãù Î¨∏ÏÑú: client.messages.create(...) :contentReference[oaicite:1]{index=1}
    try:
        from anthropic import Anthropic
    except Exception as e:
        raise LLMError(f"anthropic package import failed: {e}")
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise LLMError("ANTHROPIC_API_KEY is missing")
    
    client = Anthropic(api_key=api_key)

    files_block = _build_files_block(files)

    user_prompt = f"""\
Error log:
{error_log}

Related files:
{files_block}

Generate a unified diff that fixes the error.
Output ONLY unified diff.
"""
    try:
        # Messages API Ìò∏Ï∂ú :contentReference[oaicite:2]{index=2}
        msg = client.messages.create(
            model=cfg.model,
            max_tokens=cfg.max_tokens,
            temperature=cfg.temperature,
            system=SYSTEM_PROMPT_DIFF_ONLY,
            messages=[
                {"role": "user", "content": user_prompt},
            ],
        )
    except Exception as e:
        # Ïó¨Í∏∞Ïóê rate limit / quota / auth Îì±ÏùÑ Î™®Îëê LLMErrorÎ°ú ÎûòÌïë
        raise LLMError(str(e))

    # SDK ÏùëÎãµÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú:
    # Î≥¥ÌÜµ msg.contentÎäî Î∏îÎ°ù Î¶¨Ïä§Ìä∏(ÌÖçÏä§Ìä∏ Î∏îÎ°ù Ìè¨Ìï®)Î°ú Ïò§ÎØÄÎ°ú textÎßå Ìï©Ïπ®
    text_parts = []
    try:
        for block in (msg.content or []):
            # anthropic SDKÎäî ÌÖçÏä§Ìä∏ Î∏îÎ°ùÏóê .textÎ•º Ï†úÍ≥µÌïòÎäî ÌòïÌÉúÍ∞Ä ÏùºÎ∞òÏ†Å
            t = getattr(block, "text", None)
            if t:
                text_parts.append(t)
    except Exception:
        pass

    text = "\n".join(text_parts).strip()
    text = normalize_diff(text)
    validate_unified_diff(text)
    return text

def _gemini_generate_diff(cfg: LLMConfig, *, error_log: str, files: list[dict]) -> str:
    try:
        from google import genai
    except Exception as e:
        raise LLMError(f"google-genai import failed: {e}")

    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise LLMError("GEMINI_API_KEY is missing")

    client = genai.Client(api_key=api_key)

    files_block = _build_files_block(files)

    """
    # Î™®Îç∏ ÌôïÏù∏
    for m in client.models.list():
        print(m.name)
    """

    user_prompt = f"""\
Error log:
{error_log}

Related files:
{files_block}

Generate a unified diff that fixes the error.
Output ONLY unified diff.
"""

    try:
        resp = client.models.generate_content(
            model=cfg.model,
            contents=user_prompt,
            config={
                "temperature": cfg.temperature,
                "max_output_tokens": cfg.max_tokens,
                "system_instruction": SYSTEM_PROMPT_DIFF_ONLY,
            },
        )
    except Exception as e:
        raise LLMError(str(e))

    text = (resp.text or "").strip()
    text = normalize_diff(text)
    validate_unified_diff(text)
    return text
